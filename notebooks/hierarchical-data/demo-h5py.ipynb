{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "In this notebook we will demonstrate the use of `h5py` for writing complex data structures. This tutorial follows the [h5py documentation](https://docs.h5py.org/en/stable/quick.html) closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "We will build a python environment in the usual way. Our goal is to install the `h5py` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with Anaconda or a virtualenv\n",
    "# on MARCC: ml anaconda\n",
    "# then install with: conda update --file reqs.yaml -p ./path/to/new/env\n",
    "# then use the environment with: conda activate -p ./path/to/new/env\n",
    "# then start a notebook with: jupyter notebook\n",
    "# the reqs.yaml file contents are:\n",
    "_ = \"\"\"\n",
    "name: demo_h5py\n",
    "dependencies:\n",
    "  - python==3.8.3\n",
    "  - conda-forge::ipdb\n",
    "  - h5py\n",
    "  - notebook\n",
    "  - pip\n",
    "  - pip:\n",
    "    - scipy\n",
    "    - numpy\n",
    "    - pyyaml\n",
    "    - ruamel\n",
    "    - mpi4py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative virtual environment instructions\n",
    "_ = \"\"\"\n",
    "# get a python if you are on a cluster\n",
    "$ ml python/3.8.6\n",
    "# install a virtual environment\n",
    "$ python -m venv ./env\n",
    "# activate the environment\n",
    "$ source ./env/bin/activate\n",
    "# install packages as needed\n",
    "(env) $ pip install h5py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use cases\n",
    "\n",
    "## Case 1: Writing and reading arrays\n",
    "\n",
    "The documentation says that h5py files can hold two kinds of information. Groups work like dictionaries, and datasets work like NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_dims = (5,4,3)\n",
    "data = np.random.rand(*eg_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the docs on the create_dataset function before we start\n",
    "h5py.File.create_dataset?\n",
    "# we see that shape is required if the data is not provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new file (we always use a context manager)\n",
    "# using a context manager allows us to skip the fp.close at the end without issue\n",
    "with h5py.File('my_data.hdf5','w') as fp:\n",
    "    # create a dataset. recall that the shape and dtype are detected\n",
    "    fp.create_dataset('result_1',data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo-h5py-v2.ipynb my_data.hdf5       test\r\n",
      "demo-h5py.ipynb    richer_data.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "# check to see that the file exists using cell magic in Jupyter\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"my_data.hdf5\" (mode r)>\n",
      "['result_1']\n",
      "(5, 4, 3)\n",
      "(2, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "# read the file. it prefers a read mode\n",
    "with h5py.File('my_data.hdf5','r') as fp:\n",
    "    # pring the object (it tells us that it is an HDF5 file)\n",
    "    print(fp)\n",
    "    # the object acts like a dict\n",
    "    print(list(fp.keys()))\n",
    "    # we can check the dimensions of the result object\n",
    "    result = fp['result_1']\n",
    "    print(result.shape)\n",
    "    # we can validate the dimensions\n",
    "    if not fp['result_1'].shape==eg_dims: \n",
    "        raise ValueError('wrong dimensions!')\n",
    "    # we can slice the arrays per usual\n",
    "    subset = result[:2,...,:2]\n",
    "    print(subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Using the hierarchy\n",
    "\n",
    "In short, a hierarchical data format contains a POSIX-like filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new file\n",
    "with h5py.File('richer_data.hdf5','w') as fp:\n",
    "    # create a group\n",
    "    grp = fp.create_group('timeseries')\n",
    "    for i in range(10):\n",
    "        # generate some fake timseries data\n",
    "        ts = np.random.rand(1000,2)\n",
    "        grp.create_dataset(str(i),data=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"2\": shape (1000, 2), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "with h5py.File('richer_data.hdf5','r') as fp:\n",
    "    # create a group\n",
    "    print(fp['timeseries/2'])\n",
    "    # you can print the data by \n",
    "    # casting it: print(np.array(fp['timeseries/2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: how can we add columns to arrays?\n",
    "\n",
    "The best way to include metadata in your file is to attach it directly to an Array. You can do this by using the numpy [dtype](https://numpy.org/doc/stable/reference/generated/numpy.dtype.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we found a nice example from the docs\n",
    "np.array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example we set the string and \n",
    "# float types, along with names, for our colums\n",
    "data_with_cols = np.array(\n",
    "    [('ryan',2.5),('jane',4.0)],\n",
    "    dtype=[('student','<S4'),('grades','<f4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(b'ryan', 2.5) (b'jane', 4. )]\n",
      "('student', 'grades')\n"
     ]
    }
   ],
   "source": [
    "# the strings are converted to bytes\n",
    "print(data_with_cols)\n",
    "# we can review the column names here\n",
    "print(data_with_cols.dtype.names)\n",
    "# and this result can be stored directly with h5py\n",
    "# in the next section we add unstructured data\n",
    "# (possibly metadata) to the h5py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now add this to our file\n",
    "# the following demonstrates the append feature\n",
    "# and we also use try/except to rewrite a dataset if it exists\n",
    "with h5py.File('richer_data.hdf5','a') as fp:\n",
    "    if 'student_data' in fp: del fp['student_data']\n",
    "    fp.create_dataset('student_data',data=data_with_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['student_data', 'timeseries']\n"
     ]
    }
   ],
   "source": [
    "# check the file\n",
    "with h5py.File('richer_data.hdf5','r') as fp:\n",
    "    print(list(fp.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([b'ryan', b'jane', b'nikhil'], dtype='|S16'), array([2.5, 4. , 3.6]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is an alternate method for formulating the array\n",
    "# start with columns, ordered by rows, with distinct types\n",
    "students = np.array(['ryan','jane','nikhil']).astype('<S16')\n",
    "grades = np.array([2.5,4.0,3.6])\n",
    "students,grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[b'ryan', b'2.5'],\n",
       "       [b'jane', b'4.0'],\n",
       "       [b'nikhil', b'3.6']], dtype='|S32')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you transpose this without a dtype, h5py will not tolerate the U6 (unicode) type\n",
    "np.array(np.transpose((students,grades)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = students.shape[0]\n",
    "dt = np.dtype([('student','<S32'),('grade','<f4')])\n",
    "student_data = np.empty(n_rows,dtype=dt)\n",
    "student_data['student'] = students\n",
    "student_data['grade'] = grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'ryan', 2.5), (b'jane', 4. ), (b'nikhil', 3.6)],\n",
       "      dtype=[('student', 'S32'), ('grade', '<f4')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our data is now structured by dtype\n",
    "student_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5 4.  3.6]\n",
      "(b'jane', 4.)\n"
     ]
    }
   ],
   "source": [
    "# we can select one column or row\n",
    "print(student_data['grade'])\n",
    "print(student_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users interested in more data science-oriented structures are encouraged to check out [pandas](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Using metadata (attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate some metadata\n",
    "meta = {\n",
    "    'model':{\n",
    "        'k0':1.2,'k1':1.3,'tau':0.01,'mass':123.},\n",
    "    'integrator':{\n",
    "        'dt':0.001,'method':'rk4',},\n",
    "    'data':{\n",
    "        'source_path':'path/to/data',\n",
    "        'host':'bluecrab',},}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  host: bluecrab\n",
      "  source_path: path/to/data\n",
      "integrator:\n",
      "  dt: 0.001\n",
      "  method: rk4\n",
      "model:\n",
      "  k0: 1.2\n",
      "  k1: 1.3\n",
      "  mass: 123.0\n",
      "  tau: 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or use yaml (not part of the standard library) (pip install pyyaml)\n",
    "import yaml\n",
    "text = yaml.dump(meta)\n",
    "meta = yaml.load(text,Loader=yaml.SafeLoader)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": {\"host\": \"bluecrab\", \"source_path\": \"path/to/data\"}, \"integrator\": {\"dt\": 0.001, \"method\": \"rk4\"}, \"model\": {\"k0\": 1.2, \"k1\": 1.3, \"mass\": 123.0, \"tau\": 0.01}}\n"
     ]
    }
   ],
   "source": [
    "# serialize the data\n",
    "import json\n",
    "meta_s = json.dumps(meta)\n",
    "print(meta_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new file, this time with metadata\n",
    "with h5py.File('richer_data.hdf5','w') as fp:\n",
    "    # add the metadata\n",
    "    fp.create_dataset('meta',data=np.string_(meta_s))\n",
    "    # create a group\n",
    "    grp = fp.create_group('timeseries')\n",
    "    for i in range(10):\n",
    "        # generate some fake timseries data\n",
    "        ts = np.random.rand(1000,2)\n",
    "        grp.create_dataset(str(i),data=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  host: bluecrab\n",
      "  source_path: path/to/data\n",
      "integrator:\n",
      "  dt: 0.001\n",
      "  method: rk4\n",
      "model:\n",
      "  k0: 1.2\n",
      "  k1: 1.3\n",
      "  mass: 123.0\n",
      "  tau: 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the data\n",
    "with h5py.File('richer_data.hdf5','r') as fp:\n",
    "    #! first we get an h5py object\n",
    "    #! meta_s = fp['meta']\n",
    "    #! next we realize our docs are old\n",
    "    #! meta_s = fp['meta'].value\n",
    "    #! next we get a byte\n",
    "    #! meta_s = fp['meta'][()]\n",
    "    meta_s = fp['meta'][()].decode()\n",
    "    #! print(meta_s)\n",
    "    # unpack the data\n",
    "    meta = json.loads(meta_s)\n",
    "    print(yaml.dump(meta))\n",
    "    # get an item in the timeseries\n",
    "    result = fp['timeseries/8']\n",
    "    # cast this as an array\n",
    "    # you can view the data by\n",
    "    # casting it as an array: print(np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('richer_data.hdf5','r') as fp:\n",
    "    this = fp['meta'][()].decode()\n",
    "    meta = json.loads(this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'host': 'bluecrab',\n",
      "          'source_path': 'path/to/data'},\n",
      " 'integrator': {'dt': 0.001,\n",
      "                'method': 'rk4'},\n",
      " 'model': {'k0': 1.2,\n",
      "           'k1': 1.3,\n",
      "           'mass': 123.0,\n",
      "           'tau': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(meta,width=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Hashing your data\n",
    "\n",
    "Or, how do I organize the data? The following is a poor-man's database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you change the metadata, you change the hash\n",
    "meta['data']['host'] = 'rockfish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_8c9c7dcc57.h5py\n"
     ]
    }
   ],
   "source": [
    "import hashlib,json\n",
    "# serialize the data with some special flags\n",
    "# read about stability: https://stackoverflow.com/a/22003440\n",
    "meta_s = json.dumps(meta,\n",
    "    ensure_ascii=True,sort_keys=True,default=str)\n",
    "hashcode = hashlib.sha1(meta_s.encode()).hexdigest()[:10]\n",
    "# we can save unique files with guaranteed-unique names\n",
    "print('data_%s.h5py'%hashcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude: you can treat each file as a metaphorical row in a database. The filesystem thereby acts as a large parallel database. This saves the effort of he traditional wrappers, handlers, validation, etc, that a database requires. You only have to perform some modest file management. In the future we can discuss more fully-featured database solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
