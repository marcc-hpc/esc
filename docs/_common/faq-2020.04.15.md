---
layout: post
title: FAQ and Discussion, April 2020
---

The following documentation covers a discussion from April 2020 with the Trayanova group.

## Questions

### Q1. How to choose a partition?

> How do I select which partition to run my jobs on?  If I have a job that is only single node, is it ever appropriate to run it on parallel (perhaps if shared has a backlog)? From a resource allocation perspective, is it ok to run my jobs on skylake (if skylake has no queued jobs) if my job does not require skylake architecture?

#### Rules of thumb

1. *Architecture rarely matters.* The *Blue Crab* cluster has four different architectures from Ivy Bridge through Sky Lake. Codes which use specific, chip-level instructions for certain calculations (e.g. `AVX256`) may see a dramatic increase in speed on newer architectures and particularly GPUs (this is the *raison d'etre* for Intel and NVidia as we approach fabrication limits). Most clusters, however, do not enforce a rule regarding architectures so you are welcome to compute on any architecture. We have separated `skylake` from the rest of the cluster to improve memory management.

2. *When does architecture matter?* If you are using the large memory `lrgmem` nodes you may need to hand-compile your code for the Ivy Bridge architecture. Most codes on *Blue Crab* are compiled for the next lowest common denominator: Haswell.

3. *Use cores to request memory.* Our cluster will automatically allocate the maximum amount of memory available for each core. If you have a single-threaded calculation with a large memory requirement, then you can use `--mem` in your SLURM jobs. In all other cases, you should request more cores if you need more memory, since you can use these cores for threaded programs. The `skylake` and `express` partitions have `3853MB` per core while the rest of the cluster has `4917MB` per core except for `lrgmem` which has about `21GB` per core. You should only use `lrgmem` if you need more than `117GB` memory, which is the memory limit for most nodes.

4. Use [SLURM job arrays](https://slurm.schedmd.com/job_array.html) whenever you submit more than 100 jobs at a time.

> O.K. that was probably more information than I need. Could you be more concise?

#### Guide to choosing a partition

1. MPI (message passing interface codes) with more than 1 node must use the `parallel` partition.
2. Any code which uses an *entire* node can submit to `parallel,shared` which is the only intersection on the cluster. These jobs are the modal job on the cluster, and submitting to both reduces traffic. This is subject to change, however future rule changes will be strictly enforced
3. If your code requires up to `96GB` of memory, you can also submit one-node jobs to `skylake`. We only restrict the GPUs and large memory nodes to codes which require this hardware. Skylake can be used for any calculations.
4. If you need less than one node, use `shared` unless you require a GPU or more than `117GB` of memory.
5. If you are using less than one node on `shared`, `lrgmem`, or a GPU partition, be sure to use cores to request memory (see above).
5. Do not use the GPU or large memory nodes if your code does not require them. We cannot strictly prevent non-GPU codes from running on a GPU node, however frequent offenders will be banned.
6. Never use the `unlimited` partition without talking to me (Ryan Bradley) first.

MARCC manages resources in two ways: SLURM and the allocation. Technically, if you can request it with SLURM, then your use is allowed. However, MARCC strongly encourages all users to **benchmark** their codes in order to make efficient use of your allocation.

### Q2. Interactive sessions

> Sometimes the login nodes seem slow if I'm doing simple tasks (`mv`, `cp`, `sed`, `vim`, etc) -- should I use an interactive session on debug for those kinds of things instead?  How would I go about logging into an interactive session on debug?

#### Why are login/head nodes slow?

With rare exceptions, the login nodes are not a bottleneck. Sometimes a user will run a highly-parallel application on the head node. When we find them, we typically ban them until they can be re-trained. 

Most "slowness" is actually caused by the filesystem. The entire cluster runs on a networked filesystem, which means that running `vim` from the head node requires communication with one of many storage servers. Lag on filesystem commands can occur when their are bottlenecks on these servers. We encourage all users to minimize their use of *small files* to reduce these bottlenecks.

#### How to use `interact`

Even though our filesystem speed affects the entire cluster, it is often useful to use an interactive session to reduce traffic on the head nodes and ensure that your calculations have the right number of resources.

The `interact` command allows you to quickly book a few cores on a compute node.

~~~
interact -p express -c 6 -t 600
interact -p debug -c 4 -t 120
salloc -p gpuk80 --gres=gpu:2 -c 12 -t 120 srun --pty bash
~~~

See below for more details on interactive visualization.

### Q3. Interactive visualization

> Is there a way I can interactively plot residuals of my simulation while it is running on MARCC? The residuals are saved in a log file, and I have to copy that locally every time and run a python script to plot the residuals. 

There are two ways to perform interactive visualization.

1. Use `sshfs` to mount storage from *Blue Crab* on your local machine.
2. Use `jupyter` to visualize your data over the web.

The `sshfs` command is best for refreshing a text file or watching an automatically-generated plot with Firefox and the `file:///` utility.

~~~
mkdir ~/bluecrab
sshfs -p 22 marcc:/home-net/home-4/rbradle8@jhu.edu ~/bluecrab/ -oauto_cache,reconnect,defer_permissions,noappledouble,negative_vncache,volname=bluecrab -o follow_symlinks
~~~

It is also possible to use `Jupyter` as a full-fledged web interface to *Blue Crab*. This requires some [special instructions](https://marcc-hpc.github.io/tutorials/shortcourse_portal_jupyter.html) for connecting to the cluster, since we lack a dedicated webserver for this feature.

~~~
sbatch -p express -c 6 -t 600 jupyter_marcc notebook
sbatch -p debug -c 6 --gres=gpu:1 -t 2:0:0 jupyter_marcc notebook
~~~

To use this method:

1. Run one of the commands above. You are welcome to schedule any resource, however wait times will be lowest for `express` and `debug`. 
2. Use `sqme` to make sure the job has started. You can also request email notification.
3. Check the output log when it is available: `more slurm-43247784.out`.
4. Use the ssh command and the link provided in the SLURM output. You must run the `ssh` command from the local machine in order to use your local web browser.

In our example, I ran the following command in a separate terminal on my laptop.

~~~
ssh -N -L 9062:compute0718:9062 rbradle8@jhu.edu@login.marcc.jhu.edu
~~~

This connects me to my compute node. The link provided in the SLURM output will provide access to the Jupyter server.

### Q4. Predicting wait times

> Can you show how to use the waittime command?  How can I use it to decide which partition to submit to?  Can I use it to see how long until my queued jobs run?

The `waittime` command is a simple script that uses simple SLURM commands to compare the submission time to the start time. SLURM can sometimes predict when a job will start, so some of the reported start times might be in the future. Most of the wait time data comes from recently submitted jobs.

To look at your personal wait times for recently completed jobs, use the following syntax. Note that the `-S` flag is optional.

~~~
waittime sacct -u <username> -S 2020-04-01 | more
waittime sacct -a -r shared -S 2020-04-01 -E 2020-04-02 | more
~~~

The second command above will access historical wait times for all users on the `shared` partition. It might take a moment.

To take a look at the wait times for *currently running* and pending jobs (if SLURM can predict their start time), use the following syntax.

~~~
waittime squeue -p parallel
waittime squeue -p parallel | sort -nk7
~~~

Note that waittime simply reads the output from `sacct` and `squeue` which are well documented ([sacct](https://slurm.schedmd.com/sacct.html) and [squeue](https://slurm.schedmd.com/squeue.html)). Priority and usage are also major factors in your wait time. We can discuss this further in person.

> Which resource requests have the biggest effect on my queue position?  If I change the wall time requested for a job from 36 hours to 24 hours, does it make much difference?

You should always overestimate your wall time by a small amount, otherwise SLURM will terminate your jobs prematurely. The resources with the highest demand are the large memory nodes and (sometimes) the GPUs. A 36-hour job will absolutely wait longer than a 24-hour job, particularly since SLURM uses backfill to schedule shorter jobs while it collects resources for larger jobs. That said, the largest affect of queue position is the number of nodes, and your relative usage compared to every other user. This makes it very difficult to predict wait times.

### Q5. Where to store code

> How should we implement the new `~/code` directory? I.e should all of our run scripts and preprocessing scripts be stored here and just use file paths to where are models/pacing coordinates are stored?

The `~/code` directory is the appropriate place to store compiled programs, source code, and any files which will be frequently read during your workflow. The entire home directory is now mounted on a read-optimized filesystem to reduce stress in other parts of our data center. 

If you have text files, scripts, or interpreted code (i.e. Python, Perl, etc) which is executed more than once per minute in the aggregate, they should go in the home directory. There is no performance reason to keep your scripts separate from your data, however I strongly recommend keeping ***all codes*** under version control so you do not end up with multiple versions.

Both Lustre (`~/scratch` and `~/work`) and ZFS (`~/work-zfs` and `~/data`) are optimized for large files. Wherever possible, we should avoid using many small files because they create extra stress on almost all HPC and data center hardware.